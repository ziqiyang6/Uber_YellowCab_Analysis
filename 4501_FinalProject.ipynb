{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f7dcf-bbce-4024-80bc-3d368bb624a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 lxml\n",
    "!pip install fastparquet\n",
    "!Drd4pip install pyarrow\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f87dde-0109-4350-91ea-6eabab5e7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "from statistics import NormalDist\n",
    "import geopandas as gpd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f9a004-267b-40cc-8410-2838ac236aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_detect(file, folder_path):\n",
    "    if os.path.abspath(file) != file:\n",
    "        file = folder_path + file\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    return os.path.commonpath([file, folder_path]) == folder_path and os.path.isfile(file)\n",
    "\n",
    "\n",
    "def sample_calc(population, confi_inter, p, error):\n",
    "    n = population\n",
    "    z = NormalDist().inv_cdf((1 + confi_inter) / 2.)\n",
    "    q = 1 - p\n",
    "    e = error\n",
    "    n_0 = (z ** 2 * p * q) / (e ** 2)\n",
    "    n_final = n_0 / (1 + (n_0 - 1) / n)\n",
    "    return int(n_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62de1824-053c-47fb-9b6f-044e3f30c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "directory_path = \"/Users/olaf/Columbia_Source/IEOR_4501/final_project/\"\n",
    "directory = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58d9a4d-d033-42cb-a24c-65c17924da3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/olaf/Columbia_Source/IEOR_4501/final_project/data/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(directory_path, directory)\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42e1953-bb21-412b-9782-92864ac1ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"request failed, status code:\", response.status_code)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5672d455-316c-4474-ae1b-5f7ec22248d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_tripdata_2024-01.parquet  has been downloaded！\n",
      "yellow_tripdata_2024-02.parquet  has been downloaded！\n",
      "yellow_tripdata_2024-03.parquet  has been downloaded！\n",
      "yellow_tripdata_2024-04.parquet has been downloaded！\n",
      "yellow_tripdata_2024-05.parquet has been downloaded！\n",
      "yellow_tripdata_2024-06.parquet has been downloaded！\n",
      "yellow_tripdata_2024-07.parquet has been downloaded！\n",
      "yellow_tripdata_2024-08.parquet has been downloaded！\n",
      "yellow_tripdata_2024-09.parquet has been downloaded！\n",
      "yellow_tripdata_2023-01.parquet has been downloaded！\n",
      "yellow_tripdata_2023-02.parquet has been downloaded！\n",
      "yellow_tripdata_2023-03.parquet has been downloaded！\n",
      "yellow_tripdata_2023-04.parquet has been downloaded！\n",
      "yellow_tripdata_2023-05.parquet  has been downloaded！\n",
      "yellow_tripdata_2023-06.parquet has been downloaded！\n",
      "yellow_tripdata_2023-07.parquet  has been downloaded！\n",
      "yellow_tripdata_2023-08.parquet  has been downloaded！\n",
      "yellow_tripdata_2023-09.parquet  has been downloaded！\n",
      "yellow_tripdata_2023-10.parquet  has been downloaded！\n",
      "yellow_tripdata_2023-11.parquet  has been downloaded！\n",
      "yellow_tripdata_2023-12.parquet has been downloaded！\n",
      "yellow_tripdata_2022-01.parquet has been downloaded！\n",
      "yellow_tripdata_2022-02.parquet has been downloaded！\n",
      "yellow_tripdata_2022-03.parquet has been downloaded！\n",
      "yellow_tripdata_2022-04.parquet has been downloaded！\n",
      "yellow_tripdata_2022-05.parquet has been downloaded！\n",
      "yellow_tripdata_2022-06.parquet has been downloaded！\n",
      "yellow_tripdata_2022-07.parquet has been downloaded！\n",
      "yellow_tripdata_2022-08.parquet has been downloaded！\n",
      "yellow_tripdata_2022-09.parquet has been downloaded！\n",
      "yellow_tripdata_2022-10.parquet has been downloaded！\n",
      "yellow_tripdata_2022-11.parquet has been downloaded！\n",
      "yellow_tripdata_2022-12.parquet has been downloaded！\n",
      "yellow_tripdata_2021-01.parquet has been downloaded！\n",
      "yellow_tripdata_2021-02.parquet has been downloaded！\n",
      "yellow_tripdata_2021-03.parquet has been downloaded！\n",
      "yellow_tripdata_2021-04.parquet has been downloaded！\n",
      "yellow_tripdata_2021-05.parquet has been downloaded！\n",
      "yellow_tripdata_2021-06.parquet has been downloaded！\n",
      "yellow_tripdata_2021-07.parquet has been downloaded！\n",
      "yellow_tripdata_2021-08.parquet has been downloaded！\n",
      "yellow_tripdata_2021-09.parquet has been downloaded！\n",
      "yellow_tripdata_2021-10.parquet has been downloaded！\n",
      "yellow_tripdata_2021-11.parquet has been downloaded！\n",
      "yellow_tripdata_2021-12.parquet has been downloaded！\n",
      "yellow_tripdata_2020-01.parquet has been downloaded！\n",
      "yellow_tripdata_2020-02.parquet has been downloaded！\n",
      "yellow_tripdata_2020-03.parquet has been downloaded！\n",
      "yellow_tripdata_2020-04.parquet has been downloaded！\n",
      "yellow_tripdata_2020-05.parquet has been downloaded！\n",
      "yellow_tripdata_2020-06.parquet has been downloaded！\n",
      "yellow_tripdata_2020-07.parquet has been downloaded！\n",
      "yellow_tripdata_2020-08.parquet has been downloaded！\n",
      "yellow_tripdata_2020-09.parquet has been downloaded！\n",
      "yellow_tripdata_2020-10.parquet has been downloaded！\n",
      "yellow_tripdata_2020-11.parquet has been downloaded！\n",
      "yellow_tripdata_2020-12.parquet has been downloaded！\n",
      "fhvhv_tripdata_2024-01.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2024-02.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2024-03.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2024-04.parquet has been downloaded！\n",
      "fhvhv_tripdata_2024-05.parquet has been downloaded！\n",
      "fhvhv_tripdata_2024-06.parquet has been downloaded！\n",
      "fhvhv_tripdata_2024-07.parquet has been downloaded！\n",
      "fhvhv_tripdata_2024-08.parquet has been downloaded！\n",
      "fhvhv_tripdata_2024-09.parquet has been downloaded！\n",
      "fhvhv_tripdata_2023-01.parquet has been downloaded！\n",
      "fhvhv_tripdata_2023-02.parquet has been downloaded！\n",
      "fhvhv_tripdata_2023-03.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2023-04.parquet has been downloaded！\n",
      "fhvhv_tripdata_2023-05.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2023-06.parquet has been downloaded！\n",
      "fhvhv_tripdata_2023-07.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2023-08.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2023-09.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2023-10.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2023-11.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2023-12.parquet  has been downloaded！\n",
      "fhvhv_tripdata_2022-01.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-02.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-03.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-04.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-05.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-06.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-07.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-08.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-09.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-10.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-11.parquet has been downloaded！\n",
      "fhvhv_tripdata_2022-12.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-01.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-02.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-03.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-04.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-05.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-06.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-07.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-08.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-09.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-10.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-11.parquet has been downloaded！\n",
      "fhvhv_tripdata_2021-12.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-01.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-02.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-03.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-04.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-05.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-06.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-07.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-08.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-09.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-10.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-11.parquet has been downloaded！\n",
      "fhvhv_tripdata_2020-12.parquet has been downloaded！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "Yellow_Taxi_pattern = re.compile(r'yellow_tripdata_202[0-4]-\\d{2}.parquet', re.IGNORECASE)\n",
    "HVFHV_pattern = re.compile(r'fhvhv_tripdata_202[0-4]-\\d{2}.parquet', re.IGNORECASE)\n",
    "\n",
    "Yellow_Taxi_links = [link.get('href') for link in soup.find_all('a', href=Yellow_Taxi_pattern)]\n",
    "HVFHV_links = [link.get('href') for link in soup.find_all('a', href=HVFHV_pattern)]\n",
    "\n",
    "all_links = Yellow_Taxi_links + HVFHV_links\n",
    "\n",
    "# Download the file if it is not in the folder \n",
    "for link in all_links:\n",
    "\n",
    "    \n",
    "    file_url = link if link.startswith('http') else url + link\n",
    "    file_name = file_url.split('/')[-1]\n",
    "    abs_file_name = os.path.join(data_path, file_name)\n",
    "    if file_detect(file_name, data_path) == False:    \n",
    "        print(f\"Downloading {file_name} ...\")\n",
    "        file_response = requests.get(file_url)\n",
    "        \n",
    "        with open(abs_file_name, 'wb') as file:\n",
    "            file.write(file_response.content)\n",
    "    else:\n",
    "        print(f\"{file_name} has been downloaded！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2e9c9e-89b6-45e7-8fbc-465e3fced796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2023-12-01 00:06:06   2023-12-01 00:15:47              0.0   \n",
      "1         1  2023-12-01 00:22:26   2023-12-01 00:28:53              0.0   \n",
      "2         1  2023-12-01 00:59:44   2023-12-01 01:13:22              2.0   \n",
      "3         2  2023-12-01 00:22:17   2023-12-01 00:30:59              1.0   \n",
      "4         2  2023-12-01 00:18:16   2023-12-01 00:25:32              2.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           1.10         1.0                  N           230            48   \n",
      "1           1.50         1.0                  N           142           238   \n",
      "2           2.20         1.0                  N           114           186   \n",
      "3           0.66         1.0                  N            79            79   \n",
      "4           2.20         1.0                  N           229           263   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1         10.0    3.5      0.5        1.50           0.0   \n",
      "1             1          9.3    3.5      0.5        2.85           0.0   \n",
      "2             1         13.5    3.5      0.5        3.00           0.0   \n",
      "3             2          7.2    1.0      0.5        0.00           0.0   \n",
      "4             1         11.4    1.0      0.5        2.00           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
      "0                    1.0         16.50                   2.5          0.0  \n",
      "1                    1.0         17.15                   2.5          0.0  \n",
      "2                    1.0         21.50                   2.5          0.0  \n",
      "3                    1.0         12.20                   2.5          0.0  \n",
      "4                    1.0         18.40                   2.5          0.0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(rf'{data_path}yellow_tripdata_2023-12.parquet', engine='pyarrow')\n",
    "yellow_sample = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ee3f62-0205-41e7-bba0-b7a6ed7ef60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
      "0            HV0003               B02764               B02764   \n",
      "1            HV0003               B02764               B02764   \n",
      "2            HV0005               B02510                 None   \n",
      "3            HV0003               B02883               B02883   \n",
      "4            HV0003               B02883               B02883   \n",
      "\n",
      "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
      "0 2020-12-01 00:08:29 2020-12-01 00:11:29 2020-12-01 00:13:22   \n",
      "1 2020-12-01 00:44:34 2020-12-01 00:47:00 2020-12-01 00:47:19   \n",
      "2 2020-12-01 00:10:54                 NaT 2020-12-01 00:17:14   \n",
      "3 2020-11-30 23:58:37 2020-12-01 00:00:01 2020-12-01 00:01:16   \n",
      "4 2020-12-01 00:25:44 2020-12-01 00:30:41 2020-12-01 00:32:03   \n",
      "\n",
      "     dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  sales_tax  \\\n",
      "0 2020-12-01 00:33:53            94            75        6.90  ...       1.79   \n",
      "1 2020-12-01 00:57:01            75           164        3.73  ...       1.41   \n",
      "2 2020-12-01 00:37:42            87           179       11.73  ...       2.58   \n",
      "3 2020-12-01 00:20:26            80            92        8.34  ...       1.77   \n",
      "4 2020-12-01 00:45:18            92            92        2.77  ...       1.00   \n",
      "\n",
      "   congestion_surcharge  airport_fee  tips  driver_pay  shared_request_flag  \\\n",
      "0                  0.00          NaN  0.00       17.91                    N   \n",
      "1                  2.75          NaN  1.88        8.99                    N   \n",
      "2                  2.75          NaN  0.00       23.30                    N   \n",
      "3                  0.00          NaN  2.82       18.82                    N   \n",
      "4                  0.00          NaN  0.00        9.70                    N   \n",
      "\n",
      "   shared_match_flag  access_a_ride_flag  wav_request_flag wav_match_flag  \n",
      "0                  N                                     N              N  \n",
      "1                  N                                     N              N  \n",
      "2                  N                   N                 N              N  \n",
      "3                  N                                     N              N  \n",
      "4                  N                                     N              N  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(rf'{data_path}fhvhv_tripdata_2020-12.parquet', engine='pyarrow')\n",
    "uber_sample = pd.DataFrame(columns=df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0e927c-02b3-4b96-bd02-bfe3e4c7c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_paths = glob.glob(rf'{data_path}yellow_tripdata_202[0-4]-*.parquet')\n",
    "fhvhv_paths = glob.glob(rf'{data_path}fhvhv_tripdata_202[0-4]-*.parquet')\n",
    "uber_identifiers = ['HV0003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c84b8f3-ea1a-4599-a4e1-178f5d449a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135032467"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_pop = 0\n",
    "uber_pop = 0\n",
    "for file in yellow_paths:\n",
    "    df = pd.read_parquet(file, engine='pyarrow')\n",
    "    yellow_pop += df[df.columns[0]].count()\n",
    "    # try:\n",
    "    #     yellow_data = df[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID']]\n",
    "    #     yellow_data.rename(columns={'tpep_pickup_datetime': 'pickup_time', 'tpep_dropoff_datetime': 'dropoff_time', 'PULocationID': 'pickup_location_id', 'DOLocationID': 'dropoff_location_id'},inplace=True)\n",
    "    #     yellow_data.to_parquet(file_path, index=False, engine='pyarrow')\n",
    "    # except KeyError as e:\n",
    "    #     print('The column names have been changed')\n",
    "for file in fhvhv_paths:\n",
    "    df = pd.read_parquet(file, engine='pyarrow')\n",
    "    df = df[df['hvfhs_license_num'].isin(uber_identifiers)]\n",
    "    uber_pop += df[df.columns[0]].count()\n",
    "yellow_pop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a94953b5-aeb3-4f1a-b212-5e008dc51f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_sample.csv has been created. Loading..\n"
     ]
    }
   ],
   "source": [
    "yellow_sample_size = sample_calc(yellow_pop, 0.95, 0.5, 0.05)\n",
    "samples = []\n",
    "if file_detect('yellow_sample.csv', directory_path) == False:      \n",
    "    for file in yellow_paths:\n",
    "        df = pd.read_parquet(file, engine='pyarrow')\n",
    "        sample_data = df.sample(n=yellow_sample_size, random_state=42)\n",
    "        samples.append(sample_data)\n",
    "    yellow_sample = pd.concat(samples, ignore_index=True)\n",
    "    try:\n",
    "        yellow_sample = yellow_sample[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID']]\n",
    "    except KeyError as e:\n",
    "        print('The column names have been changed')\n",
    "    yellow_sample.to_csv(f'{directory_path}yellow_sample.csv', index=False)\n",
    "else:\n",
    "    print('yellow_sample.csv has been created. Loading..')\n",
    "    yellow_sample = pd.read_csv(f'{directory_path}yellow_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fd09a08-7582-41ba-8b8d-11ee908d4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2023-06-28 08:27:09   2023-06-28 09:21:52              1.0          10.22   \n",
      "1  2023-06-13 22:05:38   2023-06-13 22:10:48              1.0           0.80   \n",
      "2  2023-06-09 10:25:49   2023-06-09 10:41:51              2.0           1.62   \n",
      "3  2023-06-28 15:56:14   2023-06-28 17:22:03              1.0          18.90   \n",
      "4  2023-06-22 07:12:42   2023-06-22 07:23:47              1.0           1.52   \n",
      "\n",
      "   PULocationID  DOLocationID  \n",
      "0           138           144  \n",
      "1           263           237  \n",
      "2           162           236  \n",
      "3           229             1  \n",
      "4            43            74  \n"
     ]
    }
   ],
   "source": [
    "print(yellow_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1c72c46-27d4-4483-aa74-01bb59e52f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uber_sample.csv has been created. Loading..\n"
     ]
    }
   ],
   "source": [
    "uber_sample_size = sample_calc(uber_pop, 0.95, 0.5, 0.05)\n",
    "\n",
    "samples = []\n",
    "\n",
    "if file_detect('uber_sample.csv', directory_path) == False:      \n",
    "    for file in fhvhv_paths:\n",
    "        df = pd.read_parquet(file, engine='pyarrow')\n",
    "        df = df[df['hvfhs_license_num'].isin(uber_identifiers)]\n",
    "        sample_data = df.sample(n=uber_sample_size, random_state=42)\n",
    "        samples.append(sample_data)\n",
    "    uber_sample = pd.concat(samples, ignore_index=True)    \n",
    "    try: \n",
    "        uber_sample = uber_sample[['hvfhs_license_num', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time']]\n",
    "    except KeyError as e:\n",
    "        print('The column names have been changed')\n",
    "    uber_sample.to_csv(f'{directory_path}uber_sample.csv', index=False)\n",
    "else:\n",
    "    print('uber_sample.csv has been created. Loading..')\n",
    "    uber_sample = pd.read_csv(f'{directory_path}uber_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97192ac3-ac4e-40c7-b93d-da0a8e678c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hvfhs_license_num      pickup_datetime     dropoff_datetime  PULocationID  \\\n",
      "0            HV0003  2021-03-19 08:11:50  2021-03-19 08:20:59            97   \n",
      "1            HV0003  2021-03-01 22:08:28  2021-03-01 22:20:19           247   \n",
      "2            HV0003  2021-03-13 05:48:40  2021-03-13 05:55:36            18   \n",
      "3            HV0003  2021-03-12 21:02:02  2021-03-12 21:25:13           211   \n",
      "4            HV0003  2021-03-31 08:49:53  2021-03-31 09:56:02            74   \n",
      "\n",
      "   DOLocationID  trip_miles  trip_time  \n",
      "0           231        2.52        549  \n",
      "1            20        2.41        711  \n",
      "2           247        2.21        416  \n",
      "3           229        3.85       1391  \n",
      "4            45       18.96       3969  \n"
     ]
    }
   ],
   "source": [
    "print(uber_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bffa2bdd-da79-4114-a12f-75979015135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OBJECTID', 'Shape_Leng', 'Shape_Area', 'zone', 'LocationID', 'borough',\n",
      "       'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "link = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip'\n",
    "zone_zipfile = 'taxi_zones.zip'\n",
    "zone_zipfile_abs = data_path + zone_zipfile\n",
    "zone_file = 'taxi_zones.shp'\n",
    "\n",
    "if file_detect(zone_zipfile, data_path) == False:      \n",
    "    response = requests.get(link)\n",
    "    with open(zone_zipfile_abs, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "if file_detect(zone_file, data_path) == False:\n",
    "    with zipfile.ZipFile(zone_zipfile_abs, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "# 加载包含区域信息的 Shapefile 文件\n",
    "zone_gdf = gpd.read_file(f\"{data_path}taxi_zones.shp\")\n",
    "\n",
    "# 查看列名\n",
    "print(zone_gdf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13556c2d-0899-4e6b-9062-4e4bacba7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:    \n",
    "    \n",
    "    zone_gdf['centroid'] = zone_gdf.geometry.centroid\n",
    "    zone_gdf['centroid'] = zone_gdf['centroid'].to_crs(epsg=4326)\n",
    "    zone_gdf['latitude'] = zone_gdf['centroid'].y\n",
    "    zone_gdf['longitude'] = zone_gdf['centroid'].x\n",
    "    \n",
    "    zone_gdf = zone_gdf[['LocationID', 'latitude', 'longitude']]\n",
    "except AttributeError as e:\n",
    "    print('The DataFrame has been edited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ec851a74-70db-4841-b2d4-b9d55526109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LocationID   latitude  longitude\n",
      "56           56  40.751819 -73.853582\n",
      "103         103  40.698769 -74.040771\n",
      "104         103  40.688784 -74.019073\n",
      "0        1\n",
      "1        2\n",
      "2        3\n",
      "3        4\n",
      "4        5\n",
      "5        6\n",
      "6        7\n",
      "7        8\n",
      "8        9\n",
      "9       10\n",
      "10      11\n",
      "11      12\n",
      "12      13\n",
      "13      14\n",
      "14      15\n",
      "15      16\n",
      "16      17\n",
      "17      18\n",
      "18      19\n",
      "19      20\n",
      "20      21\n",
      "21      22\n",
      "22      23\n",
      "23      24\n",
      "24      25\n",
      "25      26\n",
      "26      27\n",
      "27      28\n",
      "28      29\n",
      "29      30\n",
      "30      31\n",
      "31      32\n",
      "32      33\n",
      "33      34\n",
      "34      35\n",
      "35      36\n",
      "36      37\n",
      "37      38\n",
      "38      39\n",
      "39      40\n",
      "40      41\n",
      "41      42\n",
      "42      43\n",
      "43      44\n",
      "44      45\n",
      "45      46\n",
      "46      47\n",
      "47      48\n",
      "48      49\n",
      "49      50\n",
      "50      51\n",
      "51      52\n",
      "52      53\n",
      "53      54\n",
      "54      55\n",
      "55      56\n",
      "56      56\n",
      "57      58\n",
      "58      59\n",
      "59      60\n",
      "60      61\n",
      "61      62\n",
      "62      63\n",
      "63      64\n",
      "64      65\n",
      "65      66\n",
      "66      67\n",
      "67      68\n",
      "68      69\n",
      "69      70\n",
      "70      71\n",
      "71      72\n",
      "72      73\n",
      "73      74\n",
      "74      75\n",
      "75      76\n",
      "76      77\n",
      "77      78\n",
      "78      79\n",
      "79      80\n",
      "80      81\n",
      "81      82\n",
      "82      83\n",
      "83      84\n",
      "84      85\n",
      "85      86\n",
      "86      87\n",
      "87      88\n",
      "88      89\n",
      "89      90\n",
      "90      91\n",
      "91      92\n",
      "92      93\n",
      "93      94\n",
      "94      95\n",
      "95      96\n",
      "96      97\n",
      "97      98\n",
      "98      99\n",
      "99     100\n",
      "100    101\n",
      "101    102\n",
      "102    103\n",
      "103    103\n",
      "104    103\n",
      "105    106\n",
      "106    107\n",
      "107    108\n",
      "108    109\n",
      "109    110\n",
      "110    111\n",
      "111    112\n",
      "112    113\n",
      "113    114\n",
      "114    115\n",
      "115    116\n",
      "116    117\n",
      "117    118\n",
      "118    119\n",
      "119    120\n",
      "120    121\n",
      "121    122\n",
      "122    123\n",
      "123    124\n",
      "124    125\n",
      "125    126\n",
      "126    127\n",
      "127    128\n",
      "128    129\n",
      "129    130\n",
      "130    131\n",
      "131    132\n",
      "132    133\n",
      "133    134\n",
      "134    135\n",
      "135    136\n",
      "136    137\n",
      "137    138\n",
      "138    139\n",
      "139    140\n",
      "140    141\n",
      "141    142\n",
      "142    143\n",
      "143    144\n",
      "144    145\n",
      "145    146\n",
      "146    147\n",
      "147    148\n",
      "148    149\n",
      "149    150\n",
      "150    151\n",
      "151    152\n",
      "152    153\n",
      "153    154\n",
      "154    155\n",
      "155    156\n",
      "156    157\n",
      "157    158\n",
      "158    159\n",
      "159    160\n",
      "160    161\n",
      "161    162\n",
      "162    163\n",
      "163    164\n",
      "164    165\n",
      "165    166\n",
      "166    167\n",
      "167    168\n",
      "168    169\n",
      "169    170\n",
      "170    171\n",
      "171    172\n",
      "172    173\n",
      "173    174\n",
      "174    175\n",
      "175    176\n",
      "176    177\n",
      "177    178\n",
      "178    179\n",
      "179    180\n",
      "180    181\n",
      "181    182\n",
      "182    183\n",
      "183    184\n",
      "184    185\n",
      "185    186\n",
      "186    187\n",
      "187    188\n",
      "188    189\n",
      "189    190\n",
      "190    191\n",
      "191    192\n",
      "192    193\n",
      "193    194\n",
      "194    195\n",
      "195    196\n",
      "196    197\n",
      "197    198\n",
      "198    199\n",
      "199    200\n",
      "200    201\n",
      "201    202\n",
      "202    203\n",
      "203    204\n",
      "204    205\n",
      "205    206\n",
      "206    207\n",
      "207    208\n",
      "208    209\n",
      "209    210\n",
      "210    211\n",
      "211    212\n",
      "212    213\n",
      "213    214\n",
      "214    215\n",
      "215    216\n",
      "216    217\n",
      "217    218\n",
      "218    219\n",
      "219    220\n",
      "220    221\n",
      "221    222\n",
      "222    223\n",
      "223    224\n",
      "224    225\n",
      "225    226\n",
      "226    227\n",
      "227    228\n",
      "228    229\n",
      "229    230\n",
      "230    231\n",
      "231    232\n",
      "232    233\n",
      "233    234\n",
      "234    235\n",
      "235    236\n",
      "236    237\n",
      "237    238\n",
      "238    239\n",
      "239    240\n",
      "240    241\n",
      "241    242\n",
      "242    243\n",
      "243    244\n",
      "244    245\n",
      "245    246\n",
      "246    247\n",
      "247    248\n",
      "248    249\n",
      "249    250\n",
      "250    251\n",
      "251    252\n",
      "252    253\n",
      "253    254\n",
      "254    255\n",
      "255    256\n",
      "256    257\n",
      "257    258\n",
      "258    259\n",
      "259    260\n",
      "260    261\n",
      "261    262\n",
      "262    263\n"
     ]
    }
   ],
   "source": [
    "print(zone_gdf[zone_gdf.duplicated(subset=['LocationID'])])\n",
    "print(zone_gdf['LocationID'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2efce6de-7bb3-4d6a-9cde-c83485fc215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PUDatetime</th>\n",
       "      <th>DODatetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>LocationID_dropoff</th>\n",
       "      <th>latitude_dropoff</th>\n",
       "      <th>longitude_dropoff</th>\n",
       "      <th>LocationID_pickup</th>\n",
       "      <th>latitude_pickup</th>\n",
       "      <th>longitude_pickup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-28 08:27:09</td>\n",
       "      <td>2023-06-28 09:21:52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>144</td>\n",
       "      <td>40.720889</td>\n",
       "      <td>-73.996919</td>\n",
       "      <td>138</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.873628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-13 22:05:38</td>\n",
       "      <td>2023-06-13 22:10:48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>237</td>\n",
       "      <td>40.768615</td>\n",
       "      <td>-73.965635</td>\n",
       "      <td>263</td>\n",
       "      <td>40.778766</td>\n",
       "      <td>-73.951010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-09 10:25:49</td>\n",
       "      <td>2023-06-09 10:41:51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>236</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.957012</td>\n",
       "      <td>162</td>\n",
       "      <td>40.756688</td>\n",
       "      <td>-73.972356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-28 15:56:14</td>\n",
       "      <td>2023-06-28 17:22:03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1</td>\n",
       "      <td>40.691830</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>229</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-22 07:12:42</td>\n",
       "      <td>2023-06-22 07:23:47</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>74</td>\n",
       "      <td>40.801169</td>\n",
       "      <td>-73.937346</td>\n",
       "      <td>43</td>\n",
       "      <td>40.782477</td>\n",
       "      <td>-73.965555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17928</th>\n",
       "      <td>2023-03-31 17:46:57</td>\n",
       "      <td>2023-03-31 17:56:36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>68</td>\n",
       "      <td>40.748427</td>\n",
       "      <td>-73.999918</td>\n",
       "      <td>164</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.985156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17929</th>\n",
       "      <td>2023-03-02 11:43:52</td>\n",
       "      <td>2023-03-02 11:54:43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>114</td>\n",
       "      <td>40.728340</td>\n",
       "      <td>-73.997380</td>\n",
       "      <td>90</td>\n",
       "      <td>40.742279</td>\n",
       "      <td>-73.996971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17930</th>\n",
       "      <td>2023-03-12 22:04:08</td>\n",
       "      <td>2023-03-12 22:13:13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>234</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.990458</td>\n",
       "      <td>90</td>\n",
       "      <td>40.742279</td>\n",
       "      <td>-73.996971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17931</th>\n",
       "      <td>2023-03-06 20:59:31</td>\n",
       "      <td>2023-03-06 21:05:50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>162</td>\n",
       "      <td>40.756688</td>\n",
       "      <td>-73.972356</td>\n",
       "      <td>163</td>\n",
       "      <td>40.764421</td>\n",
       "      <td>-73.977569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17933</th>\n",
       "      <td>2023-03-19 23:34:47</td>\n",
       "      <td>2023-03-19 23:56:15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.63</td>\n",
       "      <td>231</td>\n",
       "      <td>40.717773</td>\n",
       "      <td>-74.007880</td>\n",
       "      <td>138</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.873628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16721 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               PUDatetime          DODatetime  passenger_count  trip_distance  \\\n",
       "0     2023-06-28 08:27:09 2023-06-28 09:21:52              1.0          10.22   \n",
       "1     2023-06-13 22:05:38 2023-06-13 22:10:48              1.0           0.80   \n",
       "2     2023-06-09 10:25:49 2023-06-09 10:41:51              2.0           1.62   \n",
       "3     2023-06-28 15:56:14 2023-06-28 17:22:03              1.0          18.90   \n",
       "4     2023-06-22 07:12:42 2023-06-22 07:23:47              1.0           1.52   \n",
       "...                   ...                 ...              ...            ...   \n",
       "17928 2023-03-31 17:46:57 2023-03-31 17:56:36              1.0           1.29   \n",
       "17929 2023-03-02 11:43:52 2023-03-02 11:54:43              0.0           1.40   \n",
       "17930 2023-03-12 22:04:08 2023-03-12 22:13:13              5.0           0.97   \n",
       "17931 2023-03-06 20:59:31 2023-03-06 21:05:50              2.0           0.90   \n",
       "17933 2023-03-19 23:34:47 2023-03-19 23:56:15              2.0          10.63   \n",
       "\n",
       "       LocationID_dropoff  latitude_dropoff  longitude_dropoff  \\\n",
       "0                     144         40.720889         -73.996919   \n",
       "1                     237         40.768615         -73.965635   \n",
       "2                     236         40.780436         -73.957012   \n",
       "3                       1         40.691830         -74.174002   \n",
       "4                      74         40.801169         -73.937346   \n",
       "...                   ...               ...                ...   \n",
       "17928                  68         40.748427         -73.999918   \n",
       "17929                 114         40.728340         -73.997380   \n",
       "17930                 234         40.740337         -73.990458   \n",
       "17931                 162         40.756688         -73.972356   \n",
       "17933                 231         40.717773         -74.007880   \n",
       "\n",
       "       LocationID_pickup  latitude_pickup  longitude_pickup  \n",
       "0                    138        40.774376        -73.873628  \n",
       "1                    263        40.778766        -73.951010  \n",
       "2                    162        40.756688        -73.972356  \n",
       "3                    229        40.756729        -73.965146  \n",
       "4                     43        40.782477        -73.965555  \n",
       "...                  ...              ...               ...  \n",
       "17928                164        40.748575        -73.985156  \n",
       "17929                 90        40.742279        -73.996971  \n",
       "17930                 90        40.742279        -73.996971  \n",
       "17931                163        40.764421        -73.977569  \n",
       "17933                138        40.774376        -73.873628  \n",
       "\n",
       "[16721 rows x 10 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latitude and Longitude Range\n",
    "lat_min, lon_min = 40.560445, -74.242330\n",
    "lat_max, lon_max = 40.908524, -73.717047\n",
    "    \n",
    "# Match the latitude and longitude of the dataset\n",
    "if 'LocationID_pickup' not in yellow_sample:\n",
    "    yellow_sample = yellow_sample.merge(zone_gdf, how='left', left_on='PULocationID', right_on='LocationID', suffixes=('', '_pickup'))\n",
    "if 'LocationID_dropoff' not in yellow_sample:\n",
    "    yellow_sample = yellow_sample.merge(zone_gdf, how='left', left_on='DOLocationID', right_on='LocationID', suffixes=('', '_dropoff'))\n",
    " \n",
    "# Keep the rows that has different pickup location and drop location, and both pick up and drop off location id are in the zone dataframe\n",
    "yellow_sample = yellow_sample[(yellow_sample['PULocationID'] != yellow_sample['DOLocationID'])\n",
    "& (yellow_sample['trip_distance'] > 0) \n",
    "& (yellow_sample['PULocationID'] <= 263) \n",
    "& (yellow_sample['DOLocationID'] <= 263) \n",
    "]\n",
    "\n",
    "# Drop the duplicated columns\n",
    "yellow_sample = yellow_sample.loc[:,~yellow_sample.columns.duplicated()]\n",
    "\n",
    "# Change the dtype of both pick up and drop off location id to int64\n",
    "yellow_sample['LocationID_pickup'] = yellow_sample['LocationID_pickup'].astype('int64')\n",
    "yellow_sample['LocationID_dropoff'] = yellow_sample['LocationID_dropoff'].astype('int64')\n",
    "\n",
    "# Sanity Check: All the PULocationID should match pick up location id, and DOLocationID should match drop off location id\n",
    "if yellow_sample['PULocationID'].all() != yellow_sample['LocationID_pickup'].all() & yellow_sample['DOLocationID'].all() != yellow_sample['LocationID_dropoff'].all():\n",
    "    raise('Pickup Location or Dropoff Location ID not matached. Something went wrong!')\n",
    "\n",
    "# Rename the columns\n",
    "yellow_sample.rename(columns={\n",
    "    'tpep_pickup_datetime': 'PUDatetime',\n",
    "    'tpep_dropoff_datetime': 'DODatetime'\n",
    "}, inplace=True)\n",
    "\n",
    "# Set the appropriate dtype for each column\n",
    "yellow_sample['PUDatetime'] = pd.to_datetime(yellow_sample['PUDatetime'])\n",
    "yellow_sample['DODatetime'] = pd.to_datetime(yellow_sample['DODatetime'])\n",
    "yellow_sample['trip_distance'] = yellow_sample['trip_distance'].astype(float)\n",
    "\n",
    "# Keep the data only within the range \n",
    "yellow_sample = yellow_sample[\n",
    "    (yellow_sample['latitude_pickup'].between(lat_min, lat_max)) & \n",
    "    (yellow_sample['longitude_pickup'].between(lon_min, lon_max)) &\n",
    "    (yellow_sample['latitude_dropoff'].between(lat_min, lat_max)) &\n",
    "    (yellow_sample['longitude_dropoff'].between(lon_min, lon_max))\n",
    "]\n",
    "\n",
    "# Drop the columns useless columns\n",
    "yellow_sample = yellow_sample.drop(['PULocationID', 'DOLocationID', 'LocationID', 'latitude', 'longitude'], axis=1)\n",
    "\n",
    "yellow_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6a77a53a-9b0e-4365-8722-8d93a41c5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latitude and Longitude Range\n",
    "lat_min, lon_min = 40.560445, -74.242330\n",
    "lat_max, lon_max = 40.908524, -73.717047\n",
    "    \n",
    "# Match the latitude and longitude of the dataset\n",
    "if 'LocationID_pickup' not in uber_sample:\n",
    "    uber_sample = uber_sample.merge(zone_gdf, how='left', left_on='PULocationID', right_on='LocationID', suffixes=('', '_pickup'))\n",
    "if 'LocationID_dropoff' not in uber_sample:\n",
    "    uber_sample = uber_sample.merge(zone_gdf, how='left', left_on='DOLocationID', right_on='LocationID', suffixes=('', '_dropoff'))\n",
    "\n",
    "\n",
    "# Keep the rows that has different pickup location and drop location, and both pick up and drop off location id are in the zone dataframe\n",
    "uber_sample = uber_sample[(uber_sample['PULocationID'] != uber_sample['DOLocationID']) & \n",
    "(uber_sample['trip_miles'] > 0) & \n",
    "(uber_sample['PULocationID'] <= 263) & \n",
    "(uber_sample['DOLocationID'] <= 263) &\n",
    "(uber_sample[\n",
    " \n",
    "]\n",
    "\n",
    "# Drop the duplicated columns\n",
    "uber_sample = uber_sample.loc[:,~uber_sample.columns.duplicated()]\n",
    "\n",
    "# # Change the dtype of both pick up and drop off location id to int64\n",
    "# uber_sample['LocationID_pickup'] = uber_sample['LocationID_pickup'].astype('int64')\n",
    "# uber_sample['LocationID_dropoff'] = uber_sample['LocationID_dropoff'].astype('int64')\n",
    "\n",
    "# # Sanity Check: All the PULocationID should match pick up location id, and DOLocationID should match drop off location id\n",
    "# if uber_sample['PULocationID'].all() != uber_sample['LocationID_pickup'].all() & uber_sample['DOLocationID'].all() != uber_sample['LocationID_dropoff'].all():\n",
    "#     raise('Pickup Location or Dropoff Location ID not matached. Something went wrong!')\n",
    "\n",
    "# # Rename the columns\n",
    "# uber_sample.rename(columns={\n",
    "#     'tpep_pickup_datetime': 'PUDatetime',\n",
    "#     'tpep_dropoff_datetime': 'DODatetime',\n",
    "#     'trip_miles': 'trip_distance'\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Set the appropriate dtype for each column\n",
    "# uber_sample['PUDatetime'] = pd.to_datetime(yellow_sample['PUDatetime'])\n",
    "# uber_sample['DODatetime'] = pd.to_datetime(yellow_sample['DODatetime'])\n",
    "# uber_sample['trip_distance'] = uber_sample['trip_distance'].astype(float)\n",
    "\n",
    "# # Keep the data only within the range \n",
    "# uber_sample = uber_sample[\n",
    "#     (uber_sample['latitude_pickup'].between(lat_min, lat_max)) & \n",
    "#     (uber_sample['longitude_pickup'].between(lon_min, lon_max)) &\n",
    "#     (uber_sample['latitude_dropoff'].between(lat_min, lat_max)) &\n",
    "#     (uber_sample['longitude_dropoff'].between(lon_min, lon_max))\n",
    "# ]\n",
    "\n",
    "# # Drop the columns useless columns\n",
    "# uber_sample = uber_sample.drop(['PULocationID', 'DOLocationID', 'LocationID', 'latitude', 'longitude'], axis=1)\n",
    "\n",
    "# uber_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5ced0323-2640-4fee-af23-ef2f1476557a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>LocationID_dropoff</th>\n",
       "      <th>latitude_dropoff</th>\n",
       "      <th>longitude_dropoff</th>\n",
       "      <th>LocationID_pickup</th>\n",
       "      <th>latitude_pickup</th>\n",
       "      <th>longitude_pickup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>2021-09-18 21:20:24</td>\n",
       "      <td>2021-09-18 21:37:18</td>\n",
       "      <td>57</td>\n",
       "      <td>130</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.0</td>\n",
       "      <td>40.704369</td>\n",
       "      <td>-73.793981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>2020-03-19 15:22:17</td>\n",
       "      <td>2020-03-19 15:31:08</td>\n",
       "      <td>57</td>\n",
       "      <td>82</td>\n",
       "      <td>2.24</td>\n",
       "      <td>531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>40.739495</td>\n",
       "      <td>-73.877118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15627</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>2024-07-01 11:02:12</td>\n",
       "      <td>2024-07-01 11:19:40</td>\n",
       "      <td>57</td>\n",
       "      <td>16</td>\n",
       "      <td>7.74</td>\n",
       "      <td>1048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.762738</td>\n",
       "      <td>-73.773421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15715</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>2024-07-21 13:19:07</td>\n",
       "      <td>2024-07-21 13:50:07</td>\n",
       "      <td>57</td>\n",
       "      <td>222</td>\n",
       "      <td>12.17</td>\n",
       "      <td>1860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222.0</td>\n",
       "      <td>40.647527</td>\n",
       "      <td>-73.882413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hvfhs_license_num      pickup_datetime     dropoff_datetime  \\\n",
       "3006             HV0003  2021-09-18 21:20:24  2021-09-18 21:37:18   \n",
       "9545             HV0003  2020-03-19 15:22:17  2020-03-19 15:31:08   \n",
       "15627            HV0003  2024-07-01 11:02:12  2024-07-01 11:19:40   \n",
       "15715            HV0003  2024-07-21 13:19:07  2024-07-21 13:50:07   \n",
       "\n",
       "       PULocationID  DOLocationID  trip_miles  trip_time  LocationID  \\\n",
       "3006             57           130        4.88       1014         NaN   \n",
       "9545             57            82        2.24        531         NaN   \n",
       "15627            57            16        7.74       1048         NaN   \n",
       "15715            57           222       12.17       1860         NaN   \n",
       "\n",
       "       latitude  longitude  LocationID_dropoff  latitude_dropoff  \\\n",
       "3006        NaN        NaN               130.0         40.704369   \n",
       "9545        NaN        NaN                82.0         40.739495   \n",
       "15627       NaN        NaN                16.0         40.762738   \n",
       "15715       NaN        NaN               222.0         40.647527   \n",
       "\n",
       "       longitude_dropoff  LocationID_pickup  latitude_pickup  longitude_pickup  \n",
       "3006          -73.793981                NaN              NaN               NaN  \n",
       "9545          -73.877118                NaN              NaN               NaN  \n",
       "15627         -73.773421                NaN              NaN               NaN  \n",
       "15715         -73.882413                NaN              NaN               NaN  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(uber_sample.head())\n",
    "uber_sample[uber_sample['LocationID_pickup'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29863776-ed15-41db-ad60-877d8a9d2ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4bba81b7-0c30-4713-951f-180acace4cdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'daily_weather_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weather_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily_weather_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 加载每日天气数据\u001b[39;00m\n\u001b[1;32m      3\u001b[0m weather_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(weather_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m weather_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'daily_weather_data.csv'"
     ]
    }
   ],
   "source": [
    "weather_df = pd.read_csv(\"daily_weather_data.csv\")  # 加载每日天气数据\n",
    "\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "\n",
    "weather_df.set_index('date', inplace=True)\n",
    "hourly_weather_df = weather_df.resample('H').interpolate(method='linear')\n",
    "\n",
    "hourly_weather_df.to_csv(\"hourly_weather_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231eb61-f728-43e7-8b05-b4d73640d347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
